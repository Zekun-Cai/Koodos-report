# NeurIPS 2024 ｜揭示时态领域泛化本质，模型动力学开辟时域泛化新范式

## 摘要
在实际应用中，数据集的数据分布往往随着时间而不断变化，预测模型需要持续更新以保持准确性。时域泛化（Temporal Domain Generalization，TDG）旨在预测未来数据分布，从而提前更新模型，使模型与数据同步变化。然而，传统方法假设领域数据（Domain Data）在固定时间间隔内收集，忽视了现实任务中数据集采集的随机性和不定时性，无法应对数据分布在连续时间（Continuous Time）上的变化。此外，传统方法也难以保证泛化过程在整个时间流中保持稳定和可控。

为此，本文提出了连续时域泛化（Continuous Temporal Domain Generalization，CTDG）任务，并设计了一个基于模型动力学的时域泛化框架Koodos，使得模型在连续时间中与数据分布的变化始终保持协调一致。Koodos通过库普曼算子（Koopman Operator）将模型的复杂非线性动态转化为可学习的连续动力系统（Continuous Dynamical System），同时利用先验知识以确保泛化过程的稳定性和可控性。实验表明，Koodos显著超越现有方法，为时域泛化开辟了全新的研究方向。

## 作者
论文链接：https://arxiv.org/pdf/2405.16075  
代码：https://github.com/Zekun-Cai/Koodos/  
我们在代码库中提供了详细的逐步教程，涵盖Koodos的实现和可视化演示。十分钟即可快速掌握，力荐尝试！

## 情景导入

在实际应用中，训练数据的分布通常与测试数据不同，导致模型在训练环境之外的泛化能力受限。领域泛化（Domain Generalization, DG）作为一种重要的机器学习策略，旨在学习一个能够在未见目标领域中也保持良好表现的模型。近年来研究人员发现，在动态环境中，领域数据分布往往具有显著的时间依赖性，这促使了时域泛化（Temporal Domain Generalization, TDG）技术的快速发展。时域泛化将多个领域视为一个时间序列而非一组独立的静态个体，利用历史领域预测未来领域，从而实现对模型参数的提前调整，显著提升了传统DG方法的效果。

然而，现有的时域泛化研究多集中于离散时间域，即假设领域数据在固定时间间隔（如逐周或逐年）被收集。基于这一假设，概率模型被用于预测时域演变，例如通过隐变量模型生成未来数据，或利用序列模型（如LSTM）预测未来的模型参数。然而在现实中，领域数据的观测并非总是在离散、规律的时间点上，而是随机且稀疏地分布在连续时间轴上，呈现出不规律间隔的特点。图1展示了一个典型的例子——基于Twitter数据进行社交媒体舆情预测。与传统TDG中假设的领域在时间轴上规律分布不同，实际中我们只能在特定事件（如总统辩论）发生时获得一个域，而这些事件的发生时间并不固定。同时，概念漂移（Concept Drift）发生在时态域上，即领域数据分布随着时间不断演变：如活跃用户增加、新交互行为形成、年龄与性别分布变化等。理想情况下，每个时态域对应的预测模型也应随时间逐渐调整，以应对概念漂移。最后，我们关心是未来任何时刻的预测模型的状态。

事实上，领域分布在连续时间上的场景在现实中十分常见，例如：

事件驱动的数据采集：仅在特定事件发生时采集领域数据，事件之间没有数据。
流数据的随机观测：领域数据在数据流的任意时间点开始或结束采集，而非持续进行。
离散时态域但缺失：尽管领域数据基于离散时间点采集，但部分时间节点的领域数据缺失。

为了在上述场景中实现时域泛化，我们需要表征不规律时间分布域的连续动态，而这超越了传统时域泛化方法的能力范围。为此，我们提出了连续时域泛化（Continuous Temporal Domain Generalization, CTDG）任务，其中观测和未观测的领域均分布于连续时间轴上随机的时间点。CTDG旨在通过同步连续时间流中的数据动态与模型动态，使模型能够在任意指定时间点进行适应性调整，以在连续变化的环境中实现稳定可控的泛化预测。

## 核心挑战

在CTDG任务中，我们面临的挑战远超传统的TDG方法。这是因为CTDG的目标不仅在于处理不规律时间分布的训练域，更重要的是，它聚焦于如何使模型泛化到未来任意时刻。因此，CTDG必须能够在连续时间的任意时间点上——无论当时是否有领域数据——准确描述模型状态。而传统的TDG方法只需在观测时间点上优化获得当前模型状态，并将最新状态外推一步即可实现泛化。

更具体的，与TDG任务相比，CTDG的高度挑战性来自以下几个至今尚未得到充分探索的关键障碍：

- 如何建模数据动态并刻画其与模型动态的关系  
CTDG需要在不规律的时间点上刻画领域数据的连续动态，并据此同步调整模型动态。然而，数据动态本身难以直接观测，需要通过观测时间点来学习。此外，数据动态与模型动态的协调演变过程同样复杂，理解数据演变如何驱动模型演变仍然充满挑战，这构成了CTDG的首要问题。

- 如何在高维非线性模型中捕捉主要动态  
CTDG通常依赖过参数化（over-parametrized）的深度神经网络进行建模，模型在连续时间上的动态变化因此表现为高维、非线性特征。这导致模型的核心动态嵌藏于一个低维的潜空间中。如何将复杂的模型动态有效地映射到此空间中，是CTDG任务的另一项重大挑战。

- 如何确保长期泛化的稳定性和可控性  
CTDG需要泛化到未来任意时刻，这实际上要求模型具有长期泛化的稳定性。此外，在许多情况下，我们可能拥有关于动态的高层次先验知识，例如数据分布变化具有周期性。如何将这些归纳偏置嵌入CTDG的优化过程中，以提升泛化的可控性，是另一个重要的开放性问题。

## 技术方法
### 问题定义
我们研究领域数据分布随时间连续演变的泛化任务。在 CTDG 中，一个域 $`\mathcal{D}(t)`$ 表示在时间 $`t`$ 采集的数据集，由实例集 $`\{(x_i^{(t)}, y_i^{(t)})\}_{i=1}^{N(t)}`$ 组成，其中 $`x_i^{(t)} \in X(t)`$， $`y_i^{(t)} \in Y(t)`$ 和 $`N(t)`$ 分别为特征值，目标值和实例数。我们重点关注连续时间上的渐进性概念漂移，表示为领域数据的条件概率分布 $`P(Y(t) | X(t))`$ 随时间平滑变化逐步漂移。

在训练阶段，模型接收一系列在不规律时间点 $`\mathcal{T} = \{t_1, t_2, \ldots, t_T\}`$ 上收集的观测域 $`\{\mathcal{D}(t_1), \mathcal{D}(t_2), \ldots, \mathcal{D}(t_T)\}`$，其中每个时间点 $`t_i \in \mathcal{T}`$ 是定义在连续时间轴 $`\mathbb{R}^+`$ 上的实数，且满足 $`t_1 < t_2 < \ldots < t_T`$。在每个 $`t_i \in \mathcal{T}`$ 上，模型学习到一个预测函数 $`g(\cdot; \theta(t_i))`$，其中 $`\theta(t_i)`$ 表示时间 $`t_i`$ 时刻的模型参数。CTDG 的目标是建模参数的动态变化，以便在任意给定时间 $`s \notin \mathcal{T}`$ 上推断模型参数 $`\theta(s)`$，从而得到泛化模型 $`g(\cdot; \theta(s))`$。

在后续部分中，我们将使用简写符号 $`\mathcal{D}_i`$、$`X_i`$、$`Y_i`$ 和 $`\theta_i`$，分别表示在时间 $`t_i`$ 上的 $`\mathcal{D}(t_i)`$、$`X(t_i)`$、$`Y(t_i)`$ 和 $`\theta(t_i)`$。

### 设计思路
我们的方法围绕数据和模型的同步、复杂动态的简化表示，以及高效的联合优化展开，旨在克服CTDG任务中面临的核心挑战。具体思路如下：
- 同步数据与模型的动态  
  我们通过建立时间上的模型参数连续性来同步领域数据与模型动态。这一同步机制通过微分方程的形式实现，使模型参数能够随时间平滑地演变。
- 表征高维模型的低维动态  
  我们将高维模型参数映射到一个结构化的库普曼空间（Koopman Space）中。这个空间可以有效表征复杂动态，将其转化为可学习的低维线性动态。
- 联合学习模型与其动态  
  我们将单个领域的模型学习与各时间点上的连续动态进行联合优化，同时设计了归纳偏置的约束接口，确保模型能通过高效的端到端优化保持泛化的稳定性和可控性。

### 解决方案
#### Step 1. 数据动态建模与模型动态同步
- 分布变化的连续性假设  
为实现 CTDG 任务中的数据动态建模，我们首先明确数据分布在时间上的连续变化特性。我们假设条件概率分布 $`P_t(Y|X)`$ 的变化由函数 $`f`$ 描述，并逐渐演变。基于此假设，数据的渐进变化可以通过微分方程来刻画。尽管真实世界的渐进概念漂移较为复杂，但由于引发概念漂移的主要因素通常源于底层的连续过程（如自然、生物、物理、社会或经济过程），这一假设并不损失一般性。

- 模型参数的连续演化  
基于上述假设，模型参数 $` \theta(t)`$ 的演化同样可以用微分方程来描述，其动态受数据动态 $f$ 的驱动。这意味着，模型参数的变化与数据分布的变化在数学上保持同步，并满足以下微分方程：  
$`\frac{d\theta_t}{dt} = J_g(\theta_t)^{-1} f(g(\cdot; \theta_t), t)`$  
其中 $``J_g(\theta_t)``$ 为 $``g``$ 对 $``\theta_t``$ 的雅可比矩阵。具体证明详见论文定理一。  
这一方程表明，模型的参数更新过程由数据分布的连续变化驱动，其阐述了模型与数据动态的同步调整关系。  

- 优化模型动态的学习过程  
由于数据动态 $`f`$ 的具体形式未知，无法直接通过计算上式得到模型参数。为解决这一问题，我们提出一种可学习的动态函数 $`h(\theta_t, t; \phi)`$，通过训练来逼近模型参数的真实动态。  
为此，我们引入以下优化目标来学习参数 $`\phi`$，以保证参数轨迹的动态一致性： 
$`\phi = \arg \min_{\phi} \sum_{i=1}^{T} \sum_{j=1}^{i} \|\theta_i - \theta^{j \rightarrow i}_i\|_2`$  
其中 $`\theta_i`$ 通过最小化损失函数 $`\mathcal{L}(Y_i, g(X_i; \theta_i))`$ 获得，$`\theta^{j \rightarrow i}_i`$ 表示从时间 $`t_j`$ 演变至 $`t_i`$ 的积分路径： 
$`\theta^{j \rightarrow i}_i = \theta_j + \int_{t_j}^{t_i} h(\theta_{\tau}, \tau; \phi) \, d\tau`$  
通过这一优化过程，我们可以在任意时刻 $`s`$ 上求解模型参数 $`\theta(s)`$，从而确保模型在时间流中的泛化能力和动态一致性。
